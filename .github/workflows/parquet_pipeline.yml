name: Data Pipeline (Parquet)

# Parquet-based data pipeline with incremental updates and caching
# Uses GitHub Actions cache for dataset sharing between jobs

on:
  schedule:
    - cron: '0 0 * * 0'  # Sunday midnight UTC
  push:
    branches:
      - main
      - feature/parquet-pipeline
  workflow_dispatch:
    inputs:
      force_refresh:
        description: 'Force full download (ignore cache)'
        required: false
        default: false
        type: boolean

concurrency:
  group: parquet-pipeline-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  pages: write
  id-token: write
  actions: read

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '22'

jobs:
  download:
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include:
          - id: m9d7-ebf2
            name: voertuigen
            timeout: 60
          - id: sgfe-77wx
            name: meldingen
            timeout: 60
          - id: a34c-vvps
            name: geconstateerde_gebreken
            timeout: 60
          - id: hx2c-gt7k
            name: gebreken
            timeout: 10
          - id: 8ys7-d773
            name: brandstof
            timeout: 30
    timeout-minutes: ${{ matrix.timeout }}

    steps:
      - uses: actions/checkout@v4

      - name: Calculate cache key
        id: cache-key
        run: |
          WEEK=$(date +'%Y-W%V')
          echo "dataset-key=parquet-${{ matrix.name }}-$WEEK" >> $GITHUB_OUTPUT
          echo "dataset-restore-key=parquet-${{ matrix.name }}-" >> $GITHUB_OUTPUT

      - name: Restore dataset cache
        id: cache
        if: github.event.inputs.force_refresh != 'true'
        uses: actions/cache/restore@v4
        with:
          path: data/parquet/${{ matrix.name }}.parquet
          key: ${{ steps.cache-key.outputs.dataset-key }}
          restore-keys: ${{ steps.cache-key.outputs.dataset-restore-key }}

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"
          enable-cache: false

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: cd scripts && uv sync --frozen

      - name: Download dataset
        id: download
        continue-on-error: true
        run: |
          echo "Running full download for ${{ matrix.name }}"
          cd scripts && uv run python data_download.py ${{ matrix.id }} --verbose
        env:
          RDW_APP_TOKEN: ${{ secrets.RDW_APP_TOKEN }}

      - name: Verify download or use cached data
        run: |
          FILE="data/parquet/${{ matrix.name }}.parquet"
          if [ -f "$FILE" ]; then
            SIZE=$(stat -c%s "$FILE")
            echo "✓ Dataset available: $((SIZE / 1024 / 1024)) MB"
            if [ "${{ steps.download.outcome }}" = "failure" ]; then
              echo "⚠️ Fresh download failed, using cached data from previous run"
            fi
          else
            echo "✗ ERROR: No dataset available - download failed and no cache exists"
            exit 1
          fi

      - name: Save dataset cache
        if: steps.cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: data/parquet/${{ matrix.name }}.parquet
          key: ${{ steps.cache-key.outputs.dataset-key }}

  process:
    runs-on: ubuntu-latest
    needs: [download]
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4

      - name: Calculate cache keys
        id: cache-key
        run: |
          WEEK=$(date +'%Y-W%V')
          echo "voertuigen-key=parquet-voertuigen-$WEEK" >> $GITHUB_OUTPUT
          echo "meldingen-key=parquet-meldingen-$WEEK" >> $GITHUB_OUTPUT
          echo "geconstateerde-key=parquet-geconstateerde_gebreken-$WEEK" >> $GITHUB_OUTPUT
          echo "gebreken-key=parquet-gebreken-$WEEK" >> $GITHUB_OUTPUT
          echo "brandstof-key=parquet-brandstof-$WEEK" >> $GITHUB_OUTPUT

      - name: Restore voertuigen cache
        uses: actions/cache/restore@v4
        with:
          path: data/parquet/voertuigen.parquet
          key: ${{ steps.cache-key.outputs.voertuigen-key }}
          restore-keys: parquet-voertuigen-
          fail-on-cache-miss: true

      - name: Restore meldingen cache
        uses: actions/cache/restore@v4
        with:
          path: data/parquet/meldingen.parquet
          key: ${{ steps.cache-key.outputs.meldingen-key }}
          restore-keys: parquet-meldingen-
          fail-on-cache-miss: true

      - name: Restore geconstateerde_gebreken cache
        uses: actions/cache/restore@v4
        with:
          path: data/parquet/geconstateerde_gebreken.parquet
          key: ${{ steps.cache-key.outputs.geconstateerde-key }}
          restore-keys: parquet-geconstateerde_gebreken-
          fail-on-cache-miss: true

      - name: Restore gebreken cache
        uses: actions/cache/restore@v4
        with:
          path: data/parquet/gebreken.parquet
          key: ${{ steps.cache-key.outputs.gebreken-key }}
          restore-keys: parquet-gebreken-
          fail-on-cache-miss: true

      - name: Restore brandstof cache
        uses: actions/cache/restore@v4
        with:
          path: data/parquet/brandstof.parquet
          key: ${{ steps.cache-key.outputs.brandstof-key }}
          restore-keys: parquet-brandstof-
          fail-on-cache-miss: true

      - name: Verify parquet data
        run: |
          echo "Checking parquet data files:"
          ls -lh data/parquet/
          for f in voertuigen meldingen geconstateerde_gebreken gebreken brandstof; do
            if [ ! -f "data/parquet/${f}.parquet" ]; then
              echo "ERROR: Missing ${f}.parquet"
              exit 1
            fi
          done
          echo "All datasets present"

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          version: "latest"
          enable-cache: false

      - name: Set up Python
        run: uv python install ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: cd scripts && uv sync --frozen

      - name: Process data
        run: cd scripts && uv run python data_process.py

      - uses: actions/upload-artifact@v4
        with:
          name: processed-data
          path: data/processed
          retention-days: 7

  build:
    runs-on: ubuntu-latest
    needs: [process]
    timeout-minutes: 15

    steps:
      - uses: actions/checkout@v4

      - name: Download processed data
        uses: actions/download-artifact@v4
        with:
          name: processed-data
          path: data/processed

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
          cache-dependency-path: web/package-lock.json

      - name: Copy data to web public directory
        run: |
          mkdir -p web/public/data
          cp -r data/processed/* web/public/data/

      - name: Install dependencies
        working-directory: web
        run: npm ci

      - name: Build site
        working-directory: web
        run: npm run build

      - name: Setup Pages
        uses: actions/configure-pages@v5

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: web/out

  deploy:
    runs-on: ubuntu-latest
    needs: [build]
    timeout-minutes: 10
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
