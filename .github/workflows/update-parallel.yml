name: Update Car Reliability Data (Parallel)

on:
  # Run weekly on Sunday at midnight UTC (main branch only)
  schedule:
    - cron: '0 0 * * 0'
  
  # Allow manual trigger
  workflow_dispatch:
    inputs:
      force_fetch:
        description: 'Force fresh data fetch (bypass cache)'
        required: false
        default: false
        type: boolean
  
  # Run on push to main or dev branches
  push:
    branches:
      - main
      - dev
    paths:
      - 'src/**'
      - '.github/workflows/update-parallel.yml'

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: "pages-${{ github.ref_name }}"
  cancel-in-progress: true

jobs:
  # Stage 1: Fetch defects (primary dataset, needed for kentekens)
  fetch-defects:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    outputs:
      cache-hit: ${{ steps.cache-restore.outputs.cache-hit }}
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - run: pip install -r requirements.txt
      
      - name: Set sample percentage
        run: |
          if [ "${{ github.ref_name }}" = "main" ]; then
            echo "DATA_SAMPLE_PERCENT=100" >> $GITHUB_ENV
          else
            echo "DATA_SAMPLE_PERCENT=1" >> $GITHUB_ENV
          fi
      
      - name: Generate cache key
        id: cache-key
        run: |
          WEEK=$(date +%Y-W%V)
          HASH=$(sha256sum src/fetch_dataset.py | cut -c1-8)
          echo "key=defects-${{ github.ref_name }}-${WEEK}-${HASH}" >> $GITHUB_OUTPUT
      
      - name: Restore cache
        if: inputs.force_fetch != 'true'
        id: cache-restore
        uses: actions/cache/restore@v4
        with:
          path: data/defects_found.csv
          key: ${{ steps.cache-key.outputs.key }}
      
      - name: Fetch defects_found
        if: steps.cache-restore.outputs.cache-hit != 'true'
        run: python src/fetch_dataset.py defects_found
        env:
          RDW_APP_TOKEN: ${{ secrets.RDW_APP_TOKEN }}
          FETCH_WORKERS: 2
      
      - name: Save cache
        if: steps.cache-restore.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: data/defects_found.csv
          key: ${{ steps.cache-key.outputs.key }}
      
      - name: Upload defects artifact
        uses: actions/upload-artifact@v4
        with:
          name: defects_found
          path: data/defects_found.csv
          retention-days: 1

  # Stage 1b: Fetch defect codes (small, no dependencies)
  fetch-defect-codes:
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - run: pip install -r requirements.txt
      
      - name: Fetch defect_codes
        run: python src/fetch_dataset.py defect_codes
        env:
          RDW_APP_TOKEN: ${{ secrets.RDW_APP_TOKEN }}
      
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: defect_codes
          path: data/defect_codes.csv
          retention-days: 1

  # Stage 2: Fetch dependent datasets in parallel (need kentekens from defects)
  fetch-dependent:
    needs: fetch-defects
    runs-on: ubuntu-latest
    timeout-minutes: 90
    
    strategy:
      fail-fast: false
      matrix:
        dataset: [vehicles, fuel, inspections]
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - run: pip install -r requirements.txt
      
      - name: Set sample percentage
        run: |
          if [ "${{ github.ref_name }}" = "main" ]; then
            echo "DATA_SAMPLE_PERCENT=100" >> $GITHUB_ENV
          else
            echo "DATA_SAMPLE_PERCENT=1" >> $GITHUB_ENV
          fi
      
      - name: Generate cache key
        id: cache-key
        run: |
          WEEK=$(date +%Y-W%V)
          HASH=$(sha256sum src/fetch_dataset.py | cut -c1-8)
          echo "key=${{ matrix.dataset }}-${{ github.ref_name }}-${WEEK}-${HASH}" >> $GITHUB_OUTPUT
      
      - name: Restore cache
        if: inputs.force_fetch != 'true'
        id: cache-restore
        uses: actions/cache/restore@v4
        with:
          path: data/${{ matrix.dataset }}.csv
          key: ${{ steps.cache-key.outputs.key }}
      
      - name: Download defects artifact
        if: steps.cache-restore.outputs.cache-hit != 'true'
        uses: actions/download-artifact@v4
        with:
          name: defects_found
          path: data/
      
      - name: Fetch ${{ matrix.dataset }}
        if: steps.cache-restore.outputs.cache-hit != 'true'
        run: python src/fetch_dataset.py ${{ matrix.dataset }} --kentekens-from data/defects_found.csv
        env:
          RDW_APP_TOKEN: ${{ secrets.RDW_APP_TOKEN }}
          FETCH_WORKERS: 2
      
      - name: Save cache
        if: steps.cache-restore.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: data/${{ matrix.dataset }}.csv
          key: ${{ steps.cache-key.outputs.key }}
      
      - name: Upload artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.dataset }}
          path: data/${{ matrix.dataset }}.csv
          retention-days: 1

  # Stage 3: Process and deploy
  process-and-deploy:
    needs: [fetch-defects, fetch-defect-codes, fetch-dependent]
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    environment:
      name: ${{ github.ref_name == 'main' && 'github-pages' || 'dev' }}
      url: ${{ github.ref_name == 'main' && steps.deployment.outputs.page_url || '' }}
    
    steps:
      - uses: actions/checkout@v4
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - run: pip install -r requirements.txt
      
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: data/
          merge-multiple: true
      
      - name: List downloaded data
        run: ls -la data/
      
      - name: Create fetch metadata
        run: |
          if [ "${{ github.ref_name }}" = "main" ]; then
            SAMPLE=100
          else
            SAMPLE=1
          fi
          cat > data/fetch_metadata.json << EOF
          {
            "sample_percent": ${SAMPLE},
            "fetched_at": "$(date -Iseconds)",
            "workflow": "parallel"
          }
          EOF
      
      - name: Process data
        run: python src/process_data.py
      
      - name: Generate static site
        run: python src/generate_site.py
      
      - name: Setup Pages
        if: github.ref_name == 'main'
        uses: actions/configure-pages@v4
      
      - name: Upload Pages artifact
        if: github.ref_name == 'main'
        uses: actions/upload-pages-artifact@v3
        with:
          path: 'site'
      
      - name: Deploy to GitHub Pages
        if: github.ref_name == 'main'
        id: deployment
        uses: actions/deploy-pages@v4
      
      - name: Dev build summary
        if: github.ref_name == 'dev'
        run: |
          echo "## Dev Build Complete (Parallel Workflow)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Data files:" >> $GITHUB_STEP_SUMMARY
          ls -lh data/*.csv | awk '{print "- " $9 ": " $5}' >> $GITHUB_STEP_SUMMARY
